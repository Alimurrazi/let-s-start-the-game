{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split(' ')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = ' ' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\x9d': 0, '¬': 1, 'ঁ': 2, 'ং': 3, 'ঃ': 4, 'অ': 5, 'আ': 6, 'ই': 7, 'ঈ': 8, 'উ': 9, 'ঊ': 10, 'ঋ': 11, 'এ': 12, 'ঐ': 13, 'ও': 14, 'ঔ': 15, 'ক': 16, 'খ': 17, 'গ': 18, 'ঘ': 19, 'ঙ': 20, 'চ': 21, 'ছ': 22, 'জ': 23, 'ঝ': 24, 'ঞ': 25, 'ট': 26, 'ঠ': 27, 'ড': 28, 'ঢ': 29, 'ণ': 30, 'ত': 31, 'থ': 32, 'দ': 33, 'ধ': 34, 'ন': 35, 'প': 36, 'ফ': 37, 'ব': 38, 'ভ': 39, 'ম': 40, 'য': 41, 'র': 42, 'ল': 43, 'শ': 44, 'ষ': 45, 'স': 46, 'হ': 47, 'া': 48, 'ি': 49, 'ী': 50, 'ু': 51, 'ূ': 52, 'ৃ': 53, 'ে': 54, 'ৈ': 55, 'ো': 56, 'ৌ': 57, '্': 58, 'ৎ': 59, 'ৗ': 60, 'ড়': 61, 'ঢ়': 62, 'য়': 63, '\\u200c': 64, '\\u200d': 65, '\\u200f': 66, '\\ufeff': 67}\n"
     ]
    }
   ],
   "source": [
    "with open('important_data.txt') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    input_token_index=data['input_index']\n",
    "    target_token_index=data['target_index']\n",
    "    print(input_token_index)\n",
    "    for p in data['input_output']: \n",
    "        num_encoder_tokens = p['num_encoder_tokens']\n",
    "        num_decoder_tokens = p['num_decoder_tokens']\n",
    "        max_encoder_seq_length = p['max_encoder_seq_length']\n",
    "        max_decoder_seq_length = p['max_decoder_seq_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique input tokens: 68\n",
      "Number of unique output tokens: 69\n",
      "Max sequence length for inputs: 22\n",
      "Max sequence length for outputs: 22\n",
      "{'\\n': 0, ' ': 1, '\\x9d': 2, '¬': 3, 'ঁ': 4, 'ং': 5, 'ঃ': 6, 'অ': 7, 'আ': 8, 'ই': 9, 'ঈ': 10, 'উ': 11, 'ঊ': 12, 'ঋ': 13, 'এ': 14, 'ঐ': 15, 'ও': 16, 'ঔ': 17, 'ক': 18, 'খ': 19, 'গ': 20, 'ঘ': 21, 'ঙ': 22, 'চ': 23, 'ছ': 24, 'জ': 25, 'ঝ': 26, 'ঞ': 27, 'ট': 28, 'ঠ': 29, 'ড': 30, 'ঢ': 31, 'ণ': 32, 'ত': 33, 'থ': 34, 'দ': 35, 'ধ': 36, 'ন': 37, 'প': 38, 'ফ': 39, 'ব': 40, 'ভ': 41, 'ম': 42, 'য': 43, 'র': 44, 'ল': 45, 'শ': 46, 'ষ': 47, 'স': 48, 'হ': 49, 'া': 50, 'ি': 51, 'ী': 52, 'ু': 53, 'ূ': 54, 'ৃ': 55, 'ে': 56, 'ৈ': 57, 'ো': 58, 'ৌ': 59, '্': 60, 'ৎ': 61, 'ৗ': 62, 'ড়': 63, 'ঢ়': 64, 'য়': 65, '\\u200c': 66, '\\u200d': 67, '\\u200f': 68}\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "#num_encoder_tokens = len(input_characters)\n",
    "#num_decoder_tokens = len(target_characters)\n",
    "#max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "#max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "#print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "#input_token_index = dict(\n",
    "#    [(char, i) for i, char in enumerate(input_characters)])\n",
    "#target_token_index = dict(\n",
    "  #  [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "print(target_token_index)\n",
    "\n",
    "#encoder_input_data = np.zeros(\n",
    "   # (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "  #  dtype='float32')\n",
    "\n",
    "#for i, input_text in enumerate(input_texts):\n",
    " #   for t, char in enumerate(input_text):\n",
    "  #      encoder_input_data[i, t, input_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {}  \n",
    "data['people'] = []  \n",
    "data['people'].append({  \n",
    "    'name': 'Scott',\n",
    "    'website': 'stackabuse.com',\n",
    "    'from': 'Nebraska'\n",
    "})\n",
    "data['people'].append({  \n",
    "    'name': 'Larry',\n",
    "    'website': 'google.com',\n",
    "    'from': 'Michigan'\n",
    "})\n",
    "\n",
    "with open('important_data.txt', 'w') as outfile:  \n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Scott\n",
      "Website: stackabuse.com\n",
      "From: Nebraska\n",
      "\n",
      "Name: Larry\n",
      "Website: google.com\n",
      "From: Michigan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('important_data.txt') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    for p in data['people']:\n",
    "        print('Name: ' + p['name'])\n",
    "        print('Website: ' + p['website'])\n",
    "        print('From: ' + p['from'])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: প্রেমাদাসার\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: মাঠ\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: থেকে\n",
      "Decoded sentence: এলা\n",
      "\n",
      "Input sentence: শুরু\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: করে\n",
      "Decoded sentence: সে\n",
      "\n",
      "Input sentence: সামাজিক\n",
      "Decoded sentence: মালান\n",
      "\n",
      "Input sentence: যোগাযোগমাধ্যম\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: সবখানেই\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: চলছে\n",
      "Decoded sentence: এলা\n",
      "\n",
      "Input sentence: নাগিন\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: কাল\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: রাতে\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: শ্রীলঙ্কার\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: বিপক্ষে\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: অবিস্মরণীয়\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: সেই\n",
      "Decoded sentence: সে\n",
      "\n",
      "Input sentence: জয়ের\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: পর\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: মাঠেই\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: নাগিন\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: নেচেছেন\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: বাংলাদেশ\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: দলের\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: খেলোয়াড়\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: থেকে\n",
      "Decoded sentence: এলা\n",
      "\n",
      "Input sentence: টিম\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: ম্যানেজমেন্টের\n",
      "Decoded sentence: ব্রান্র\n",
      "\n",
      "Input sentence: অনেকে\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: খেলোয়াড়দের\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: এই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: সংক্রমিত\n",
      "Decoded sentence: বার্যা\n",
      "\n",
      "Input sentence: হয়েছে\n",
      "Decoded sentence: এলা\n",
      "\n",
      "Input sentence: দেশের\n",
      "Decoded sentence: সা\n",
      "\n",
      "Input sentence: ক্রিকেটপ্রেমীদের\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: মধ্যেও\n",
      "Decoded sentence: বার্যা\n",
      "\n",
      "Input sentence: যে\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: যাঁর\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: মতো\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: করে\n",
      "Decoded sentence: সে\n",
      "\n",
      "Input sentence: নাগিন\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: নাচের\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: ছবি\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: পোস্ট\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: করেছেন\n",
      "Decoded sentence: কাল\n",
      "\n",
      "Input sentence: সামাজিক\n",
      "Decoded sentence: মালান\n",
      "\n",
      "Input sentence: যোগাযোগমাধ্যমে\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: এমনকি\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: এই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: বিশ্ব\n",
      "Decoded sentence: বার্যা\n",
      "\n",
      "Input sentence: মিডিয়ার\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: নজরেও\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: পড়েছে\n",
      "Decoded sentence: ্যা\n",
      "\n",
      "Input sentence: কিন্তু\n",
      "Decoded sentence: সার্রা\n",
      "\n",
      "Input sentence: এই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: নাগিন\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: এর\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: জনক\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: কে\n",
      "Decoded sentence: সে\n",
      "\n",
      "Input sentence: এই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: নেচে\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: মুশফিকুর\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: রহিম\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: বিখ্যাত\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: হলেও\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: এটি\n",
      "Decoded sentence: এলে\n",
      "\n",
      "Input sentence: আসলে\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: নাজমুল\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: ইসলামের\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: আবিষ্কার\n",
      "Decoded sentence: সার্যা\n",
      "\n",
      "Input sentence: নাজমুল\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: কাল\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: বাংলাদেশের\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: একাদশে\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: ছিলেন\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: কিন্তু\n",
      "Decoded sentence: সার্রা\n",
      "\n",
      "Input sentence: কোনো\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: একটা\n",
      "Decoded sentence: এলা\n",
      "\n",
      "Input sentence: কারণে\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: তাঁকে\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: বোলিং\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: দেননি\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: অধিনায়ক\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: সাকিব\n",
      "Decoded sentence: সানা\n",
      "\n",
      "Input sentence: আল\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: হাসান\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: ফিল্ডিংয়ের\n",
      "Decoded sentence: সানা\n",
      "\n",
      "Input sentence: সময়\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: বেশ\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: কয়েকবারই\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: তাঁর\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: কাছে\n",
      "Decoded sentence: সা\n",
      "\n",
      "Input sentence: বল\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: গিয়েছিল\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: ব্যাটিংয়ে\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: নামার\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: সুযোগ\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: পাননি\n",
      "Decoded sentence: বার্র\n",
      "\n",
      "Input sentence: তবে\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: শেষ\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: ব্যাটসম্যান\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: হিসেবে\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: প্যাড\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: ট্যাড\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: পরে\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: অপেক্ষাতেই\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: ছিলেন\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: বিপিএলে\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: এই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: নেচে\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: সবার\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: দৃষ্টি\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: কাড়েন\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: নাজমুল\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: সর্বশেষ\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: বিপিএলে\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: এই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: নাগিন\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: মোটামুটি\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: বিখ্যাত\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: বানিয়ে\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: দেন\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: তিনি\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: উইকেট\n",
      "Decoded sentence: এলা\n",
      "\n",
      "Input sentence: পেলেই\n",
      "Decoded sentence: কা\n",
      "\n",
      "Input sentence: মাথার\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: ওপর\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: হাত\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: তুলে\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: সাপের\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: ভঙ্গিতে\n",
      "Decoded sentence: বার্রা\n",
      "\n",
      "Input sentence: এঁকেবেঁকে\n",
      "Decoded sentence: এলাল\n",
      "\n",
      "Input sentence: নাজমুলের\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: সেই\n",
      "Decoded sentence: সে\n",
      "\n",
      "Input sentence: উদ্‌যাপনের\n",
      "Decoded sentence: বার্যা\n",
      "\n",
      "Input sentence: ভঙ্গিই\n",
      "Decoded sentence: বার্র\n",
      "\n",
      "Input sentence: ছড়িয়ে\n",
      "Decoded sentence: এলা\n",
      "\n",
      "Input sentence: পড়েছে\n",
      "Decoded sentence: ্যা\n",
      "\n",
      "Input sentence: বাংলাদেশ\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: দলে\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: গত\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: বিপিএলেই\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: সংবাদকর্মীরা\n",
      "Decoded sentence: সার্রা\n",
      "\n",
      "Input sentence: নাজমুলকে\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: প্রশ্ন\n",
      "Decoded sentence: প্রান্যা\n",
      "\n",
      "Input sentence: করেছিলেন\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: এই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: নাচের\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: কী\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: রহস্য\n",
      "Decoded sentence: বার্যা\n",
      "\n",
      "Input sentence: তাঁর\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: জবাব\n",
      "Decoded sentence: সানা\n",
      "\n",
      "Input sentence: আগেরবার\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: রাজশাহী\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: কিংসে\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: খেলার\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: সময়\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: ড্যারেন\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: স্যামিকে\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: স্নেকগিরি\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: সাপের\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: দেখালে\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: ভয়\n",
      "Decoded sentence: কা\n",
      "\n",
      "Input sentence: পেত\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: মজাও\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: পেত\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: ওর\n",
      "Decoded sentence: কে\n",
      "\n",
      "Input sentence: কাছ\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: থেকেই\n",
      "Decoded sentence: এলে\n",
      "\n",
      "Input sentence: শুরু\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: তারপর\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: ম্যাচে\n",
      "Decoded sentence: ব্রান্র\n",
      "\n",
      "Input sentence: করতে\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: করতে\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: এখন\n",
      "Decoded sentence: এলেল\n",
      "\n",
      "Input sentence: হয়ে\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: গেছে\n",
      "Decoded sentence: এলে\n",
      "\n",
      "Input sentence: এভাবেই\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: আসছে\n",
      "Decoded sentence: সাল\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: নাজমুলের\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: এই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: নাগিন\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: ত্রিদেশীয়\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: সিরিজ\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: দিয়ে\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: জায়গা\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: করে\n",
      "Decoded sentence: সে\n",
      "\n",
      "Input sentence: নিয়েছে\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: আন্তর্জাতিক\n",
      "Decoded sentence: সার্যান\n",
      "\n",
      "Input sentence: মঞ্চে\n",
      "Decoded sentence: বার্রা\n",
      "\n",
      "Input sentence: বৈশ্বিক\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: সংবাদমাধ্যমও\n",
      "Decoded sentence: মান্রা\n",
      "\n",
      "Input sentence: এই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: নাচকে\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: লুফে\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: নিয়েছে\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: তবে\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: শ্রীলঙ্কা\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: দলের\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: সর্বশেষ\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: বাংলাদেশ\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: সফরে\n",
      "Decoded sentence: কা\n",
      "\n",
      "Input sentence: সিলেটে\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: এই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: দিয়ে\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: উদ্‌যাপন\n",
      "Decoded sentence: বার্যা\n",
      "\n",
      "Input sentence: সেরেছিলেন\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: গুনাতিলকা\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: কিন্তু\n",
      "Decoded sentence: সার্রা\n",
      "\n",
      "Input sentence: নিদাহাস\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: ট্রফিতে\n",
      "Decoded sentence: প্রা\n",
      "\n",
      "Input sentence: ব্যাপারটা\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: অন্য\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: মাত্রা\n",
      "Decoded sentence: মান্রা\n",
      "\n",
      "Input sentence: পেয়েছে\n",
      "Decoded sentence: মা\n",
      "\n",
      "Input sentence: বাংলাদেশ\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: দলের\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: খেলোয়াড়েরা\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: তো\n",
      "Decoded sentence: বি\n",
      "\n",
      "Input sentence: বটেই\n",
      "Decoded sentence: ল\n",
      "\n",
      "Input sentence: শ্রীলঙ্কা\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: দলের\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: খেলোয়াড়েরাও\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: উদ্‌যাপন\n",
      "Decoded sentence: বার্যা\n",
      "\n",
      "Input sentence: সারতে\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: নাগিন\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: নাচের\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: দ্বারস্থ\n",
      "Decoded sentence: স্রান্যা\n",
      "\n",
      "Input sentence: হয়েছেন\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: শুধু\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: তাই\n",
      "Decoded sentence: সা\n",
      "\n",
      "Input sentence: নয়\n",
      "Decoded sentence: সে\n",
      "\n",
      "Input sentence: প্রেমাদাসার\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: গ্যালারিতে\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: দর্শকদের\n",
      "Decoded sentence: বার্যা\n",
      "\n",
      "Input sentence: মধ্যেও\n",
      "Decoded sentence: বার্যা\n",
      "\n",
      "Input sentence: সাড়া\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: ফেলেছে\n",
      "Decoded sentence: এলাল\n",
      "\n",
      "Input sentence: এই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: শ্রীলঙ্কার\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: বিপক্ষে\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: নিজেদের\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: প্রথম\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: ম্যাচটা\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: অবিশ্বাস্য\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: নৈপুণ্যে\n",
      "Decoded sentence: বার্যা\n",
      "\n",
      "Input sentence: জেতানোর\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: পর\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: মুশফিকের\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: সেই\n",
      "Decoded sentence: সে\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: ভুলে\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: যাওয়া\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: অসম্ভব\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: কাল\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: সেই\n",
      "Decoded sentence: সে\n",
      "\n",
      "Input sentence: একই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: দলের\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: বিপক্ষে\n",
      "Decoded sentence: প্রান্র\n",
      "\n",
      "Input sentence: আরেকটি\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: অবিস্মরণীয়\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: জয়\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: তুলে\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: নেওয়ার\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: পর\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: নেচেছে\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: পুরো\n",
      "Decoded sentence: বার\n",
      "\n",
      "Input sentence: দল\n",
      "Decoded sentence: সা\n",
      "\n",
      "Input sentence: নেচেছে\n",
      "Decoded sentence: মাল\n",
      "\n",
      "Input sentence: গোটা\n",
      "Decoded sentence: সা\n",
      "\n",
      "Input sentence: বাংলাদেশ\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: বাংলাদেশের\n",
      "Decoded sentence: সালা\n",
      "\n",
      "Input sentence: দেখানো\n",
      "Decoded sentence: মালা\n",
      "\n",
      "Input sentence: এই\n",
      "Decoded sentence: এএএএএ\n",
      "\n",
      "Input sentence: নাচ\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: দেখে\n",
      "Decoded sentence: এলা\n",
      "\n",
      "Input sentence: এখন\n",
      "Decoded sentence: এলেল\n",
      "\n",
      "Input sentence: নাচছে\n",
      "Decoded sentence: সাল\n",
      "\n",
      "Input sentence: গোটা\n",
      "Decoded sentence: সা\n",
      "\n",
      "Input sentence: ক্রিকেট\n",
      "Decoded sentence: প্রান\n",
      "\n",
      "Input sentence: দুনিয়াই\n",
      "Decoded sentence: মালা\n",
      "\n",
      "292\n",
      "প্রেমাদাসার\n"
     ]
    }
   ],
   "source": [
    "# Restore the model and construct the encoder and decoder.\n",
    "model = load_model('s2s.h5')\n",
    "\n",
    "encoder_inputs = model.input[0]   # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output   # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]   # input_2\n",
    "decoder_state_input_h = Input(shape=(latent_dim,), name='input_3')\n",
    "decoder_state_input_c = Input(shape=(latent_dim,), name='input_5')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "# Decodes an input sequence.  Future work should support beam search.\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[' ']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "def get_input_data(test_txt):\n",
    "    #txt = input()\n",
    "    txt=test_txt\n",
    "    \n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(txt), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "    \n",
    "    txt = txt.replace(' ', '')\n",
    "    \n",
    "    for t, char in enumerate(txt):\n",
    "        encoder_input_data[0, t, input_token_index[char]] = 1.\n",
    "        \n",
    "    return encoder_input_data\n",
    "\n",
    "\n",
    "word_list=[]\n",
    "data_path='newspaper(khela).txt'\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    #lines = f.read().split(' ')\n",
    "    for word in f.read().split():\n",
    "        word_list.append(word)\n",
    "    for word in word_list:\n",
    "        input_seq = get_input_data(word)\n",
    "        decoded_sentence = decode_sequence(input_seq)\n",
    "        print('Input sentence:',   word)\n",
    "        print('Decoded sentence:', decoded_sentence)\n",
    "        \n",
    "print(len(word_list))        \n",
    "print(word_list[0])\n",
    "\n",
    "#for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    " #   input_seq = get_input_data(test_input_texts[i])\n",
    " #   input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "  #  decoded_sentence = decode_sequence(input_seq)\n",
    "   # print('-')\n",
    "   # print('Input sentence:', input_texts[seq_index])\n",
    "  #  print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print(epochs)\n",
    "# Run training\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history=model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kire\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 66 and 63. Shapes are [66,1024] and [63,1024]. for 'Assign_8' (op: 'Assign') with input shapes: [66,1024], [63,1024].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    687\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 66 and 63. Shapes are [66,1024] and [63,1024]. for 'Assign_8' (op: 'Assign') with input shapes: [66,1024], [63,1024].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b006614dc5ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_model_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# load weights into new model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test_model.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loaded model from disk\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[0;32m   2643\u001b[0m                 f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[0;32m   2644\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2645\u001b[1;33m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'close'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers)\u001b[0m\n\u001b[0;32m   3164\u001b[0m                              ' elements.')\n\u001b[0;32m   3165\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3166\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   2363\u001b[0m                 assign_placeholder = tf.placeholder(tf_dtype,\n\u001b[0;32m   2364\u001b[0m                                                     shape=value.shape)\n\u001b[1;32m-> 2365\u001b[1;33m                 \u001b[0massign_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2366\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2367\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking)\u001b[0m\n\u001b[0;32m    592\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m     \"\"\"\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[0;32m    274\u001b[0m     return gen_state_ops.assign(\n\u001b[0;32m    275\u001b[0m         \u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    277\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m     61\u001b[0m         \u001b[1;34m\"Assign\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         use_locking=use_locking, name=name)\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   3160\u001b[0m         op_def=op_def)\n\u001b[0;32m   3161\u001b[0m     self._create_op_helper(ret, compute_shapes=compute_shapes,\n\u001b[1;32m-> 3162\u001b[1;33m                            compute_device=compute_device)\n\u001b[0m\u001b[0;32m   3163\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[1;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   3206\u001b[0m     \u001b[1;31m# compute_shapes argument.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3207\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3208\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3209\u001b[0m     \u001b[1;31m# TODO(b/XXXX): move to Operation.__init__ once _USE_C_API flag is removed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3210\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2425\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs_c_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2426\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2427\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_set_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2398\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2400\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2401\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2402\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2329\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2330\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2332\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, require_shape_fn)\u001b[0m\n\u001b[0;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m                                   require_shape_fn)\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 66 and 63. Shapes are [66,1024] and [63,1024]. for 'Assign_8' (op: 'Assign') with input shapes: [66,1024], [63,1024]."
     ]
    }
   ],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "from keras.models import model_from_json\n",
    "\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "print(\"kire\")\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"test_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "#model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#model = load_model('s2s.h5')\n",
    "\n",
    "input_characters=66 #here Number of unique input tokens(characters): 66\n",
    "num_encoder_tokens = 66\n",
    "num_decoder_tokens=67\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[' ']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_input_data(test_txt):\n",
    "    #txt = input()\n",
    "    txt=test_txt\n",
    "    \n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(txt), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "    \n",
    "    txt = txt.replace(' ', '')\n",
    "    \n",
    "    for t, char in enumerate(txt):\n",
    "        encoder_input_data[0, t, input_token_index[char]] = 1.\n",
    "        \n",
    "    return encoder_input_data\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Take one sequence (part of the training set)\n",
    "# for trying out decoding.\n",
    "#input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    test_lines = f.read().split('\\n')\n",
    "\n",
    "test_input_texts=[]\n",
    "test_target_texts=[]\n",
    "for line in test_lines[3000: 3100]:\n",
    "    #print(line)\n",
    "    test_input_text, test_target_text = line.split(' ')\n",
    "    #print(test_input_text)\n",
    "    test_input_texts.append(test_input_text)\n",
    "    test_target_texts.append(test_target_text+'\\n')\n",
    "    \n",
    "#print(test_input_texts)   \n",
    "\n",
    "total_test_input=0\n",
    "total_accuracy=0\n",
    "\n",
    "for i in range(50):\n",
    "    total_test_input=total_test_input+1\n",
    "    input_seq = get_input_data(test_input_texts[i])\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    if (decoded_sentence==test_target_texts[i]):\n",
    "        total_accuracy=total_accuracy+1\n",
    "        #print('hoiche')\n",
    "        \n",
    "    print('Input sentence:',   test_input_texts[i])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    #print(decoded_sentence[6])\n",
    "    print('Target sentence:',  test_target_texts[i])\n",
    "    #print(test_target_texts[i][6])\n",
    "  \n",
    "    \n",
    "print('total_test_input:', total_test_input)\n",
    "print('total_accuracy:', total_accuracy)\n",
    "print('result:', (total_accuracy/total_test_input)*100,'%')\n",
    "\n",
    "#yhat = model.predict(input_seq, verbose=0)\n",
    "#print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
